## `````````````````````````````````````````````
#### Read Me ####
## `````````````````````````````````````````````

# building on 
# SRC: https://inbo.github.io/tutorials/data-handling-large-files-R.html

# However, when working with really large CSV files, you do not want to load the
# entire file into memory first (this is the whole point of this tutorial). An
# alternative strategy is to load the data from the CSV file in chunks (small
# sections) and write them step by step to the SQlite database.
# 
# This can be implemented by reading the CSV file in small sections (letâ€™s say
# 50000 lines each time) and move all sections to a given table in a sqlite
# database. As this is a recurrent task, we will provide the transformation in a
# custom written function, called csv_to_sqlite. The documentation of the
# individual input parameters of the function are explained in the documentation
# section just above the functionn itself. As SQlite does not natively support
# date and datetime representations, the function converts those columns to an
# appropriate string representation before copying the dates to sqlite. To check
# for the date handling, the lubridate package is used.

# v 1: a) exploring read_csv_chunked instead of read_csv

# R Pointers
# https://rstudio-pubs-static.s3.amazonaws.com/202583_aa19399e91204242b547339535a51a3d.html
## `````````````````````````````````````````````

## Input file
# from: reduce data v 5.R
# which in turn need libraries and global definitions from: v 0 1 7.R
c.csv.temp.file = "reduced trip data - 201501 - 2.csv" # directly saved in sublime
c.csv.temp.file = file.path(c.home.dir,c.data.dir,c.csv.temp.file)

# df.temp = read_file(c.csv.temp.file) %>%
#   str_replace_all('"{2,3}', '"') %>%
#   read_csv(col_names = TRUE,
#            col_types = cols_only(tripduration = col_integer(), 
#                                  starttime = col_datetime("%d/%m/%Y %H:%M"),
#                                  "start station id" = col_integer()))

fn_append_to_sqlite <- function(x, pos) {
  #dbWriteTable(con, table_name, x, append = TRUE)
  #str(x)
  
  #df.1 = x
  
  # 5. fix col names for df.1
  names(x) = gsub(" ", "_", names(x))
  
  # 6. remove all but first and last rows
  # TODO: remove this, as we want complete data, not just two rows
  x <-
    x %>%
    filter(row_number() %in% c(1, n()))
  
  # 7. remove all but few cols
  x <-
    x %>%
    select(starttime, start_station_id, tripduration)
  
  print.data.frame(x)
  
  # insert into db
  # dbWriteTable(con, table_name, x, append = TRUE)
  
  
}

i.chunk_size = 3

# df.temp.1 = read_file(c.csv.temp.file) %>%
#   str_replace_all('"{2,3}', '"') %>%
#   read_csv_chunked(
#     col_names = TRUE,
#     #callback = fn_append_to_sqlite,
#     callback = DataFrameCallback$new(fn_append_to_sqlite),
#     chunk_size = i.chunk_size,
#     col_types = cols_only(
#       tripduration = col_integer(),
#       starttime = col_datetime("%d/%m/%Y %H:%M"),
#       "start station id" = col_integer()
#     )
#   )

# c.files generated by
# c.files = fn_filesToDownload()
c.1 = file.path(c.home.dir,c.data.dir,basename(c.files[1]))
read_csv(c.1,col_names=TRUE,n_max=2, progress=TRUE)
read_csv_chunked(c.1,col_names=TRUE,callback=fn_append_to_sqlite, progress=TRUE)



## we only need to add two paramters 
# from read_csv to read_csv_chunked
# i.e. callback and chunk_size